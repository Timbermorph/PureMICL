# ğŸ”¬ Multimodal In-Context Learning with Unsloth

This project implements training and inference pipelines for multimodal reasoning tasks using [Unsloth](https://github.com/unslothai/unsloth), built on top of `Qwen2-VL` and LoRA-based fine-tuning.

---

## âœ… Installation

### 1ï¸âƒ£ Clone this repository

```bash
git clone https://github.com/your_org/your_repo.git
cd your_repo
```
---

### 2ï¸âƒ£ Create virtual environment & activate

```bash
python3.11 -m venv unsloth_venv
source unsloth_venv/bin/activate
```
---

### 3ï¸âƒ£ Install all dependencies

```bash
pip install --upgrade pip
pip install -r pip_requirements.txt
```
---

### âš ï¸ Important: Unsloth version must match your PyTorch, CUDA, and GPU architecture

Unsloth must be installed with the correct tag depending on your hardware and environment. For example:

**âœ… Our working configuration:**
- GPU: A100 (Ampere)
- CUDA: 12.1
- PyTorch: 2.5.1

Use the following in `pip_requirements.txt`:

```txt
unsloth[cu121-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git
```

If you use a different GPU or CUDA version, refer to [Unsloth install guide](https://github.com/unslothai/unsloth) and adjust accordingly.

---

## ğŸ“¦ Dataset Preparation

Please download the required datasets from our data release and place them under the `dataset/` folder. For example:

```bash
dataset/
â”œâ”€â”€ operator_induction/
â”‚   â”œâ”€â”€ support.json
â”‚   â””â”€â”€ query.json
â”œâ”€â”€ sudoku/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ shapes_count/
â”‚   â”œâ”€â”€ ...
...
```
---

## ğŸ§ª Running the Code

You can run training or inference using the **unified shell script**:

```bash
bash run_main.sh [infer|lora_infer|finetune]
```

| Mode         | Description                                 |
|--------------|---------------------------------------------|
| `finetune`   | Finetune the model on your training dataset |
| `infer`      | Run inference with base model               |
| `lora_infer` | Run inference with fine-tuned model    |

> âš ï¸ Only one mode can be used at a time.  
> âš ï¸ Before running, **make sure to manually set the following variables inside `run_main.sh`**:

```bash
MODEL_PATH=/absolute/path/to/your/base_model
LORA_MODEL_PATH=./ckpt/your_lora_checkpoint  # if using lora_infer mode
```

---

### âœ… Output and Evaluation

After inference, results are saved under `./results/` in JSON format.

To calculate accuracy:

```bash
python check_accuracy.py ./results/your_result_file.json
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ ckpt/                         # Your LoRA checkpoints
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ dataset/                      # Place downloaded datasets here
â”‚   â”œâ”€â”€ operator_induction/
â”‚   â”œâ”€â”€ sudoku/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ qwen2_vl_for_replacement/    # Optional model override modules
â”œâ”€â”€ check_accuracy.py
â”œâ”€â”€ data_processing.py
â”œâ”€â”€ modeling_qwen2_vl.py
â”œâ”€â”€ pip_requirements.txt
â”œâ”€â”€ qwen2_finetune_new_model.py
â”œâ”€â”€ run_infer.sh                  # Entry script for training/inference
â”œâ”€â”€ samples_of_training_data.json
â””â”€â”€ readme.txt                   # (This README content)
```

---

## ğŸ“š References

- [Unsloth: Fast and Efficient Fine-tuning](https://github.com/unslothai/unsloth)
- [Qwen2-VL on HuggingFace](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)
